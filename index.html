<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning">
  <meta name="keywords" content="Human Perception, In-Context Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <span class="highlight">S</span>keleton-<span class="highlight">i</span>n-<span class="highlight">C</span>ontext: Unified Skeleton Sequence Modeling with In-Context Learning
          </h1>
            <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xinshun Wang<sup>1</sup>*,
              Zhongbin Fang<sup>1</sup>*,
              Xia Li<sup>2</sup>,
              Xiangtai Li<sup>3</sup>, 
              Chen Chen<sup>4</sup>,
              Mengyuan Liu<sup>5</sup><i class="fas fa-envelope"></i> <!-- 添加邮箱图标 -->
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Sun Yat-sen University&nbsp;</span>
            <span class="author-block"><sup>2</sup>Department of Computer Science, ETH Zurich&nbsp;</span>
            <span class="author-block"><sup>3</sup>S-Lab, Nanyang Technological University&nbsp;</span>
            <span class="author-block"><sup>4</sup>Center for Research in Computer Vision, University of Central Florida&nbsp;</span>
            <span class="author-block"><sup>5</sup>National Key Laboratory of General Artificial Intelligence, Shenzhen Graduate School, Peking University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/fanglaosi/Skeleton-in-Context"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2312.03703.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/Necolizer/ISTA-Net"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/fanglaosi/Skeleton-in-Context"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Download Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/Necolizer/ISTA-Net"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-download"></i>
                  </span>
                  <span>Checkpoint</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        General Interactive Actions Addressing the Diversity of Interacting Entities.
        <br>
        (Person-to-person, Hand-to-hand & Hand-to-object)
      </h2>
    </div>
  </div>
</section> -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/media/Teaser_v2_00.jpg" alt="Teaser Image" height="100%">
      <h2 class="subtitle has-text-centered">
        Our work is the first to explore in-context learning for unifying skeleton sequence modeling.
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">        
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In-context learning provides a new perspective for multi-task modeling for vision and NLP.
            Under this setting, the model can perceive tasks from prompts and accomplish them without any extra task-specific head predictions or model finetuning.
            However, Skeleton sequence modeling via in-context learning remains unexplored.
            Directly applying existing in-context models from other areas onto skeleton sequences fails due to the inter-frame and cross-task pose similarity that makes it outstandingly hard to perceive the task correctly from a subtle context.
          </p>
          <p>
            To address this challenge, we propose Skeleton-in-Context (SiC), an effective framework for in-context skeleton sequence modeling. Our SiC is able to handle multiple skeleton-based tasks simultaneously after a single training process and accomplish each task from context according to the given prompt. It can further generalize to new, unseen tasks according to customized prompts.
            To facilitate context perception, we additionally propose a task-unified prompt, which adaptively learns tasks of different natures, such as partial joint-level generation, sequence-level prediction, or 2D-to-3D motion prediction.
            We conduct extensive experiments to evaluate the effectiveness of our SiC on multiple tasks, including motion prediction, pose estimation, joint completion, and future pose estimation. We also evaluate its generalization capability on unseen tasks such as motion-in-between. These experiments show that our model achieves state-of-the-art multi-task performance and even outperforms single-task methods on certain tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <video id="1" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/1.mp4"
                  type="video/mp4">
          </video>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <!-- Definition. -->
        <h2 class="title is-3">Skeleton-in-Context</h2>
        <!-- <div class="content has-text-justified">
          
        </div> -->
        <img id="teaser" src="./static/media/Framework_00.jpg" alt="Teaser Image" height="100%">


        <div class="content has-text-justified">
          <b>Overall framework of our Skeleton-in-Context.</b> Specifically, we establish a skeleton bank by integrating training sets under different tasks, which contain a large amount of input-target pairs performing different tasks.
          Next, we randomly select a sample pair as the task-guided prompt (TGP) and a query input from the skeleton bank, undergo encoding, and concatenating, respectively, and then input them into the transformer in parallel. 
          In particular, during this process, the query input and task-unified prompt (TUP) are combined to form a new query. After iterating n1 times, the TGP and query are aggregated through aggregation function and then input into the transformer for n2 iterations. 
          Lastly, the second half of the model output is used as our prediction.
        </div>
        
        <!--/ Definition. -->

        <br><br>

        <!-- Architecture. -->
        <h2 class="title is-3">Features</h2>
        <div class="content has-text-justified">
          <b>In-context learning for unifying skeleton sequence modeling</b>
            <ul>
              <li>The first work to explore the application of in-context learning in skeleton sequences.</li>
              <li>A new framework for tackling multiple tasks (four tasks), which outperforms other multi-task-capable models.</li>
              <li>Skeleton-in-Context solves the overfitting problem that is encountered when using the masked-modeling-style training framework of existing methods.</li>
            </ul>
          <b>New benchmark for skeleton-based multi-tasking</b>
            <ul>
              <li>A new multi-task benchmark for evaluating the capability of processing multiple skeleton-based tasks, including motion prediction, pose estimation, joint completion, and future pose estimation.</li>
            </ul>
          <b>Impressive performance and strong generalization capability</b>
            <ul>
              <li>Surpasses other multi-task-capable models re-structured from multi-stage models or task-specific models.</li>
              <li>Surpasses even task-specific models (siMLPe, EqMotion, STCFormer, GLA-GCN, MotionBERT) on some tasks.</li>
              <li>Skeleton-in-Context can generalize to new datasets well, and perform unseen tasks when given customized prompts.</li>
            </ul>
        </div>
        <!--/ Architecture. -->

        <!-- Benchmarks. -->
        <h2 class="title is-3">Benchmark Difficulty</h2>
        <img src="./static/images/Benchmarks.svg" alt>
        <!-- <div class="content has-text-justified">
        
        </div> -->
        <!--/ Benchmarks. -->

        <br><br>

        <!-- Visualiztions. -->
        <h2 class="title is-3">Visualizations</h2>
        <img src="./static/images/Visualiztions.svg" alt>
        <!-- <div class="content has-text-justified">
        </div> -->
        <!--/ Visualiztions. -->
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{wang2023skeleton,
        title={Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning},
        author={Wang, Xinshun and Fang, Zhongbin and Li, Xia and Li, Xiangtai and Chen, Chen and Liu, Mengyuan},
        journal={arXiv preprint arXiv:2312.03703},
        year={2023}
      }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="https://github.com/Necolizer/ISTA-Net">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/Necolizer/ISTA-Net" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
        <div class="content">
          <p>
            Template of this page is borrowed from <a
              href="https://nerfies.github.io/">Nerfies</a>. Grateful to this great work.

          </p>
        </div>
    </div>
  </div>
</footer>

</body>
</html>
